{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install twilio\n",
        "!pip install pyngrok\n",
        "!pip install streamlit\n",
        "!pip install langchain\n",
        "!pip install  \"openai<=0.28.1\"\n",
        "!pip install streamlit\n",
        "!pip install streamlit PyPDF2\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install urllib\n",
        "!pip install selenium\n",
        "\n",
        "!pip install pypdf          #loader for pdfs\n",
        "!pip install unstructured   #loader for unstructured text data\n",
        "!pip install pypandoc       #universal document converter\n",
        "!pip install chromadb       #Vector database\n",
        "\n",
        "!pip install tiktoken       #tokenizer for Open AI models"
      ],
      "metadata": {
        "id": "h2S3F6-xih2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webhook_url='https://plucky-field-buckthornpepperberry.glitch.me'\n",
        "verify_token='testing'\n",
        "whatsapp_token='EAAS3f1cqKMYBOzHJUyHwCFtJP6IxIIWfA8K2a1tD2POq8pOBiRNHFU5Ub0kPMdZBI8vovrDj1grTERmZBwoeEAIWZBQiQywmMrfBvajB6kkJrsRAyoxgQNq4uyZBSaR3ZAC36yWIWhbG0ojhhBCsjwiiC5QDahPrYVZCj9BSlMGp5qvix8uhdBksUCipZCNPRlVInMLSbvHNydvEKojFKycYZBPLyZAQZD'"
      ],
      "metadata": {
        "id": "s7nupxyHCOv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import key\n",
        "import streamlit as st\n",
        "import openai\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from PyPDF2 import PdfFileReader\n",
        "from PyPDF2 import PdfReader\n",
        "import io\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import textwrap\n",
        "\n",
        "#Libraries for Document Splitting, embeddings and vector stores\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "\n",
        "#Chain for Q&A after retrieving external documents\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "#Using ChatOpenAI\n",
        "from langchain.chat_models import ChatOpenAI            #used for GPT3.5/4 model\n",
        "\n",
        "apikey = key.OPENAI_API_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "st.title(\"Whatsapp Bot Generator\")\n",
        "biz_info=\"\"\n",
        "\n",
        "\n",
        "# Function to extract all links from a URL\n",
        "def extract_links(url, domain_name):\n",
        "    links = set()\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check if the request was successful\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            if urlparse(href).netloc == domain_name:\n",
        "                links.add(urljoin(url, href))\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching links from {url}: {e}\")\n",
        "\n",
        "    return links\n",
        "\n",
        "# Function to fetch content from a URL\n",
        "def fetch_page_content(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check if the request was successful\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        text = soup.get_text(strip=True)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching page: {e}\"\n",
        "\n",
        "# Function to extract all links from a URL and fetch their content\n",
        "def extract_links_and_content(url):\n",
        "    links_content = {}\n",
        "    domain_name = urlparse(url).netloc\n",
        "\n",
        "    # Include the main page in the processing\n",
        "    links_content[url] = fetch_page_content(url)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            if urlparse(href).netloc == domain_name:\n",
        "                full_url = urljoin(url, href)\n",
        "                if full_url not in links_content:\n",
        "                    content = fetch_page_content(full_url)\n",
        "                    links_content[full_url] = content\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching links from {url}: {e}\")\n",
        "\n",
        "    return links_content\n",
        "\n",
        "# Function to recursively extract all link\n",
        "def recursive_crawl(url, domain_name, links_content, max_depth=1, current_depth=0):\n",
        "    if current_depth > max_depth or url in links_content:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        content = fetch_page_content(url)\n",
        "        links_content[url] = content\n",
        "\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            href = link['href']\n",
        "            if urlparse(href).netloc == domain_name:\n",
        "                full_url = urljoin(url, href)\n",
        "                recursive_crawl(full_url, domain_name, links_content, max_depth, current_depth + 1)\n",
        "\n",
        "    except Exception as e:\n",
        "        links_content[url] = f\"Error fetching page: {e}\"\n",
        "\n",
        "\n",
        "#Give customers the option to upload their informtion\n",
        "with st.sidebar:\n",
        "    functionality = st.radio(\"Please provide information about yourself\", (\"Manual Description\", \"PDF Uploader\", 'Website Url'))\n",
        "\n",
        "#If customers want to write a description of themselves\n",
        "if functionality == \"Manual Description\":\n",
        "  with st.form(\"my_form\"):\n",
        "    description = st.text_input(\"Please Provide Information About Your Business\")\n",
        "    # Every form must have a submit button.\n",
        "    submitted = st.form_submit_button(\"Submit\")\n",
        "\n",
        "    if submitted:\n",
        "        biz_info=biz_info+str(description)+ \"\\n\\n\"\n",
        "\n",
        "#If customers want to upload a pdf about themselves\n",
        "elif functionality == \"PDF Uploader\":\n",
        "    uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\")\n",
        "    if uploaded_file is not None:\n",
        "        bytes_data = uploaded_file.getvalue()\n",
        "\n",
        "        try:\n",
        "            pdf_reader = PdfReader(io.BytesIO(bytes_data))\n",
        "            st.write(f\"Number of pages in the PDF: {len(pdf_reader.pages)}\")\n",
        "            for page in pdf_reader.pages:\n",
        "                biz_info=biz_info+ str(page.extract_text())+ \"\\n\\n\"\n",
        "            # Add more processing here if needed\n",
        "            submitted=True\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading PDF: {e}\")\n",
        "\n",
        "#If customers want to upload an url to their site\n",
        "elif functionality == \"Website Url\":\n",
        "    with st.form(\"webpage_form\"):\n",
        "        url = st.text_input(\"Enter the webpage URL\")\n",
        "        max_depth = st.number_input(\"Enter max depth for crawling (be cautious)\", min_value=1, max_value=20, value=1)\n",
        "        fetch_button = st.form_submit_button(\"Fetch Webpage and Subpages\")\n",
        "\n",
        "        if fetch_button and url:\n",
        "            all_content = {}\n",
        "            domain_name = urlparse(url).netloc\n",
        "            recursive_crawl(url, domain_name, all_content, max_depth)\n",
        "\n",
        "            if all_content:\n",
        "                submitted=True\n",
        "                st.write(f\"Found content from {len(all_content)} pages in the domain.\")\n",
        "                for link, content in all_content.items():\n",
        "                    st.subheader(link)\n",
        "                    # Use the link itself as a unique key for the text area\n",
        "                    #st.text_area(\"Content\", content, height=150, key=link)\n",
        "                    biz_info=biz_info+str(content)+ \"\\n\\n\"\n",
        "\n",
        "\n",
        "if biz_info:\n",
        "    #st.write(biz_info)\n",
        "\n",
        "\n",
        "    #Splitting Documents into Chunks for embeddings and the store them in vector stores\n",
        "    # chunksize and chunkoverlap are key parameters to ensure that things work\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500)\n",
        "    chunks = text_splitter.create_documents([biz_info])\n",
        "    #st.write(chunks[0])\n",
        "    st.write('\\n\\n')\n",
        "    #st.write(chunks[1])\n",
        "    #Store the chunks as embeddings within a vector store\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vector_store = Chroma.from_documents(chunks, embeddings)\n",
        "\n",
        "\n",
        "    # initialize OpenAI instance and set up a chain for Q&A from an LLM and a vector score\n",
        "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
        "    retriever=vector_store.as_retriever()\n",
        "    chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
        "\n",
        "\n",
        "    question = \"What is location of willoughbys?\"\n",
        "    st.write(question)\n",
        "    st.write('\\n\\n')\n",
        "    response = chain.run(question)\n",
        "    st.write(textwrap.fill(response,75))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lAFLLkyrVKC",
        "outputId": "3a61fa45-c26c-4a0e-fc6e-378df16cf074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "TiydZbY_jmFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2Z5l9wJJ9z2r7DfI9FXS3sZRXAz_4c3m8d5GxWdgJEH7jtHQH"
      ],
      "metadata": {
        "id": "1diqFYcQjvbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4d2feb-dcef-4aa5-83fd-0f738f7ad6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzY56HLQkoSq",
        "outputId": "9d3103d0-89cf-4f03-8369-f7d76b815ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok http 8501 --domain=llmtest.ngrok.io"
      ],
      "metadata": {
        "id": "EdOP2dayj2IB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}